# 大语言模型的限制 - 非计算能力

## 核心概念

虽然大语言模型（LLM）在生成文本、翻译语言和回答问题方面表现出色，但它们在**数学计算**和**精确统计**任务上往往表现不佳。

这是一个根本性的架构限制，而非暂时的训练不足。

## 为什么 LLM 不会“计算”？

### 1. 文字接龙机制 (Next Token Prediction)
LLM 的本质是一个概率模型，它的工作是根据上文预测下一个 Token（词或字）。
当你问它 `1 + 1 = ?` 时，它回答 `2`，不是因为它在 CPU 中运行了加法运算，而是因为它在训练数据中见过无数次 `1 + 1 = 2` 的文本模式。

### 2. Tokenizer 的干扰
LLM 并不直接“看”到字符，而是看到 Token。
例如，单词 `Strawberry` 在某些 Tokenizer 中可能被切分为 `Straw` + `berry`。当你问它“Strawberry 有几个 r？”时，模型无法像人类一样逐个字母数数，因为它看到的 Token 中并不包含直接的字母数量信息。

## 典型错误案例

### 大数运算谬误
对于简单的乘法表内的计算，模型通常能答对（因为背过）。但对于复杂的、训练数据中罕见的大数运算，模型往往会开始“胡编乱造”。
*   **输入**: `23894 * 1203`
*   **LLM**: 可能输出一个看起来像答案的数字，但最后几位通常是错的。

### 字符统计与逻辑陷阱
*   **输入**: `单词 "Strawberry" 中有几个 "r"？`
*   **LLM (常见错误)**: `2个`。
    *   *原因*：模型依赖概率直觉，而非逻辑计数。

## 解决方案

既然 LLM 自身不具备计算引擎，如何解决这个问题？

**答案：使用工具（Tool Use / Function Calling）。**

现代 AI 系统（如 ChatGPT Plus, Claude, Gemini）在遇到计算问题时，不会强行用语言模型去“猜”答案，而是会：
1.  **识别**这是一个计算任务。
2.  **编写**一段 Python 代码或调用计算器 API。
3.  **运行**代码得到精确结果。
4.  **返回**结果给用户。

这就是**Agent（智能体）**的核心思想：让 LLM 做它擅长的（理解意图、规划任务），让计算器做它擅长的（精确计算）。
